- title: "[SEV:high] Add Quickstart/Getting Started section"
  labels: ["audit", "docs", "severity:high"]
  body: |
    File: README.md
    Canonical Statement: README lacks Quickstart/Getting Started instructions, blocking adoption.
    Impact: 3
    Confidence: 0.97
    Sources: GPT-5 DOCS-001; Gemini READ-002; Claude DOC-005
    Fix: Add Python version, pinned deps, install commands, minimal run producing an artifact.

- title: "[SEV:high] Provide reproducibility/audit evidence"
  labels: ["audit", "reproducibility", "severity:high"]
  body: |
    File: README.md
    Canonical Statement: README claims auditability but provides no concrete procedures or metrics.
    Impact: 3
    Confidence: 0.85
    Sources: Claude DOC-002; Claude DOC-007
    Fix: Add metrics, reference audit templates in /templates, or include sample audits.

- title: "[SEV:medium] Add runnable examples/tutorials"
  labels: ["audit", "docs", "severity:medium"]
  body: |
    File: README.md
    Canonical Statement: README mentions case studies but provides no runnable examples/tutorials.
    Impact: 2
    Confidence: 0.90
    Sources: GPT-5 DOCS-002; Claude DOC-004
    Fix: Add runnable example with logs or Customer Review case study.

- title: "[SEV:medium] Link all referenced docs/templates"
  labels: ["audit", "docs", "severity:medium"]
  body: |
    File: README.md
    Canonical Statement: References to internal files are not hyperlinked.
    Impact: 2
    Confidence: 0.85
    Sources: GPT-5 DOCS-003
    Fix: Convert references to Markdown links.

- title: "[SEV:medium] Expand validation step examples"
  labels: ["audit", "docs", "severity:medium"]
  body: |
    File: README.md
    Canonical Statement: Validation step is abstract; no domain examples.
    Impact: 2
    Confidence: 0.85
    Sources: Claude DOC-003
    Fix: Add validation examples per domain (data analysis, literature review, code audits).

- title: "[SEV:medium] Add skill evidence/examples"
  labels: ["audit", "docs", "severity:medium"]
  body: |
    File: README.md
    Canonical Statement: Skills section lacks concrete supporting examples.
    Impact: 2
    Confidence: 0.90
    Sources: Claude DOC-001
    Fix: Add short case study results tied to each skill.

- title: "[SEV:low] Improve roadmap readability"
  labels: ["audit", "docs", "severity:low"]
  body: |
    File: README.md
    Canonical Statement: Roadmap presented as text bullets with no completion indicators.
    Impact: 1
    Confidence: 0.70
    Sources: GPT-5 DOCS-004; Claude DOC-006
    Fix: Convert to table with Phase | Status | ETA | Artifact.

- title: "[SEV:low] Convert guarantees to checklist format"
  labels: ["audit", "docs", "severity:low"]
  body: |
    File: README.md
    Canonical Statement: In-repo Guarantees are plain bullets; checklist format clearer.
    Impact: 1
    Confidence: 0.75
    Sources: GPT-5 DOCS-005
    Fix: Convert to checklist with clear verification markers.

- title: "[SEV:low] Concise introductory hook"
  labels: ["audit", "docs", "severity:low"]
  body: |
    File: README.md
    Canonical Statement: Intro section is verbose, reducing impact.
    Impact: 1
    Confidence: 0.90
    Sources: Gemini READ-001
    Fix: Rephrase into one-paragraph hook + single-sentence value prop.

- title: "[SEV:low] Make image references robust"
  labels: ["audit", "docs", "severity:low"]
  body: |
    File: README.md
    Canonical Statement: Banner image path may fail on some renderers.
    Impact: 1
    Confidence: 0.50
    Sources: Claude DOC-008
    Fix: Use absolute GitHub URLs or confirm path consistency.

- title: "[SEV:nit] Remove non-standard HTML and repetition"
  labels: ["audit", "docs", "severity:nit"]
  body: |
    File: README.md
    Canonical Statement: README uses non-standard HTML alignment and repeats reproducibility themes.
    Impact: 1
    Confidence: 0.75
    Sources: GPT-5 DOCS-006; Gemini READ-003
    Fix: Remove <p align='center'>, consolidate reproducibility/logging into one section.
